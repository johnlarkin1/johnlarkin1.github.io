<!--
  Example interactive Python block for the Muon post
  Copy and paste this into your _posts/2025-10-28-understanding-muon.md

  Suggested location: After the "Visualization" section (line 102)
-->

<div class="interactive-python">
```python
import numpy as np
import matplotlib.pyplot as plt

# Simple demonstration of SGD vs SGD with Momentum
# This shows why momentum helps optimization

# Define a simple 2D loss function (elongated bowl)
def compute_loss(x, y):
    return 0.5 * x**2 + 5 * y**2

def compute_gradient(x, y):
    dx = x
    dy = 10 * y
    return np.array([dx, dy])

# SGD without momentum
def run_sgd(start, steps=50, lr=0.05):
    path = [start.copy()]
    pos = start.copy()

    for _ in range(steps):
        grad = compute_gradient(pos[0], pos[1])
        pos = pos - lr * grad
        path.append(pos.copy())

    return np.array(path)

# SGD with momentum
def run_momentum(start, steps=50, lr=0.05, beta=0.9):
    path = [start.copy()]
    pos = start.copy()
    velocity = np.zeros(2)

    for _ in range(steps):
        grad = compute_gradient(pos[0], pos[1])
        velocity = beta * velocity - lr * grad
        pos = pos + velocity
        path.append(pos.copy())

    return np.array(path)

# Run both optimizers
start_point = np.array([10.0, 3.0])
sgd_path = run_sgd(start_point)
momentum_path = run_momentum(start_point)

# Create visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Left plot: Contour plot with paths
x = np.linspace(-12, 12, 100)
y = np.linspace(-4, 4, 100)
X, Y = np.meshgrid(x, y)
Z = compute_loss(X, Y)

ax1.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)
ax1.plot(sgd_path[:, 0], sgd_path[:, 1], 'o-',
         color='#ff6b6b', linewidth=2, markersize=3, label='SGD')
ax1.plot(momentum_path[:, 0], momentum_path[:, 1], 's-',
         color='#4ecdc4', linewidth=2, markersize=3, label='SGD + Momentum')
ax1.plot(0, 0, '*', color='#ffd93d', markersize=20,
         label='Global Minimum', zorder=5)
ax1.set_xlabel('Parameter θ₁', fontsize=12)
ax1.set_ylabel('Parameter θ₂', fontsize=12)
ax1.set_title('Optimizer Trajectories on Loss Landscape', fontsize=14, pad=15)
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.2)

# Right plot: Loss over iterations
sgd_losses = [compute_loss(p[0], p[1]) for p in sgd_path]
momentum_losses = [compute_loss(p[0], p[1]) for p in momentum_path]

ax2.plot(sgd_losses, 'o-', color='#ff6b6b',
         linewidth=2, markersize=4, label='SGD')
ax2.plot(momentum_losses, 's-', color='#4ecdc4',
         linewidth=2, markersize=4, label='SGD + Momentum')
ax2.set_xlabel('Iteration', fontsize=12)
ax2.set_ylabel('Loss', fontsize=12)
ax2.set_title('Loss Convergence Comparison', fontsize=14, pad=15)
ax2.set_yscale('log')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.2)

plt.tight_layout()

# Print summary
print("=" * 50)
print("Optimization Summary")
print("=" * 50)
print(f"Starting point: θ = [{start_point[0]:.1f}, {start_point[1]:.1f}]")
print(f"Starting loss: {compute_loss(start_point[0], start_point[1]):.2f}")
print()
print(f"SGD Final Loss: {sgd_losses[-1]:.6f}")
print(f"Momentum Final Loss: {momentum_losses[-1]:.6f}")
print()
print(f"Improvement: {(sgd_losses[-1] / momentum_losses[-1]):.1f}x faster convergence!")
print()
print("Key Insight: Momentum smooths the optimization path and")
print("accelerates convergence in directions of consistent gradient.")
```
</div>

<!-- Alternative: simpler gradient descent example -->
<div class="interactive-python">
```python
import numpy as np
import matplotlib.pyplot as plt

# Visualize the update equations for SGD and Momentum
print("Standard Gradient Descent (SGD):")
print("  θ_{t+1} = θ_t - η∇L(θ_t)")
print()
print("SGD with Momentum:")
print("  v_{t+1} = βv_t - η∇L(θ_t)")
print("  θ_{t+1} = θ_t + v_{t+1}")
print()

# Example: how velocity accumulates over time
iterations = 20
beta = 0.9
lr = 0.1
gradient = 2.0  # constant gradient for illustration

velocity = 0
velocities = [velocity]

for i in range(iterations):
    velocity = beta * velocity - lr * gradient
    velocities.append(velocity)

# Plot velocity accumulation
plt.figure(figsize=(10, 5))
plt.plot(velocities, 'o-', linewidth=2, markersize=6)
plt.axhline(y=-(lr * gradient)/(1-beta),
            color='r', linestyle='--',
            label=f'Terminal velocity = {-(lr*gradient)/(1-beta):.2f}')
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('Velocity', fontsize=12)
plt.title('How Momentum Builds Up (β=0.9, η=0.1, ∇L=2.0)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()

print(f"After {iterations} iterations:")
print(f"  Velocity: {velocities[-1]:.4f}")
print(f"  Terminal velocity: {-(lr*gradient)/(1-beta):.4f}")
print()
print("Notice: Velocity grows exponentially then stabilizes!")
```
</div>
